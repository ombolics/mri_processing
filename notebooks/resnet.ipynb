{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score, f1_score, recall_score\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from early_stopping import EarlyStopping\n",
    "import multiprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numbers\n",
    "import os\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar divide\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(scheduler_name, optimizer, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to get a specific learning rate scheduler instance.\n",
    "\n",
    "    Args:\n",
    "        scheduler_name (str): Name of the desired scheduler type.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer instance.\n",
    "        **kwargs: Additional keyword arguments to be passed to the scheduler constructor.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.lr_scheduler._LRScheduler: Instance of the specified learning rate scheduler.\n",
    "    \"\"\"\n",
    "    if scheduler_name == 'StepLR':\n",
    "        return lr_scheduler.StepLR(optimizer, **kwargs)\n",
    "    elif scheduler_name == 'MultiStepLR':\n",
    "        return lr_scheduler.MultiStepLR(optimizer, **kwargs)\n",
    "    elif scheduler_name == 'ExponentialLR':\n",
    "        return lr_scheduler.ExponentialLR(optimizer, **kwargs)\n",
    "    elif scheduler_name == 'CosineAnnealingLR':\n",
    "        return lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, **kwargs)\n",
    "    elif scheduler_name == 'ReduceLROnPlateau':\n",
    "        return lr_scheduler.ReduceLROnPlateau(optimizer, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler name: {scheduler_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(epoch,metric_type, loss, labels, predictions):\n",
    "    return {\n",
    "        \"epoch\": [epoch],\n",
    "        \"metric_type\": [metric_type],\n",
    "        \"accuracy\": [accuracy_score(labels, predictions)],\n",
    "        \"precision\": [precision_score(labels, predictions, average=\"weighted\")],\n",
    "        \"recall\": [recall_score(labels, predictions, average=\"weighted\")],\n",
    "        \"f1\": [f1_score(labels, predictions, average=\"weighted\")],\n",
    "        \"loss\" : [loss]\n",
    "    }\n",
    "\n",
    "def log_metrics(writer, metrics):\n",
    "    # Log metrics to TensorBoard\n",
    "    for key, value in metrics.items():\n",
    "        if key != \"epoch\" and key != \"metric_type\":  # Exclude non-numeric values\n",
    "            writer.add_scalar(f\"{metrics['metric_type'][0]}/{key}\", value[0], metrics['epoch'][0] + 1)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, output_path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(os.paht.join(output_path, 'confusion_matrix.jpg'))\n",
    "    plt.show()\n",
    "\n",
    "def visualize_metrics(metrics_df, output_path, title):\n",
    "    # Define the metrics to visualize\n",
    "    metrics = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 6.5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each metric in a subplot\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        sns.lineplot(data=metrics_df, x='epoch', y=metric, hue='metric_type', style='metric_type', ax=ax)\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        #ax.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.suptitle(f'Training results\\n{title}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'metrics.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 2,\n",
    "    'train_batch_size' : 256,\n",
    "    'val_batch_size' : 128,\n",
    "    'num_workers': 8,\n",
    "    'lr' : 0.001,\n",
    "    'adam_eps' : 1e-8,\n",
    "    'adam_beta1' : 0.9,\n",
    "    'adam_beta2' : 0.999,\n",
    "    'adam_weight_decay': 0.01,\n",
    "    'lr_scheduler_type': 'CosineAnnealingLR',\n",
    "\n",
    "    'es_patience': 3,\n",
    "    'es_delta': 0,\n",
    "    'checkpt_steps':2,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the number of available CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "num_cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load datasets\n",
    "train_dataset = datasets.ImageFolder('../data/kaggle_brain_tumor/Training', transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder('../data/kaggle_brain_tumor/Testing', transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args['train_batch_size'], shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args['val_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, criterion, test_loader, device, output_path):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_test_predictions.extend(predicted.cpu().numpy())\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    plot_confusion_matrix(all_test_labels, all_test_predictions, classes=test_dataset.classes, output_path=output_path)\n",
    "    test_metrics = calculate_metrics(1, 'test', test_loss, all_test_labels, all_test_predictions)\n",
    "    return {k: v[0] for k, v in test_metrics.items() if isinstance(v[0], numbers.Number)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(run_name, model, criterion, optimizer, device, args):\n",
    "    # Tensorboard writer\n",
    "    timestamped_run_name = (\n",
    "        f\"{run_name}_{datetime.datetime.now()}\".replace(\":\", \"_\")\n",
    "        .replace(\".\", \"_\")\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace('-','_')\n",
    "    )\n",
    "    output_path = os.path.join('runs',timestamped_run_name)\n",
    "    checkpoint_path = os.path.join(output_path, \"checkpoints\")\n",
    "    best_checkpt_path = os.path.join(checkpoint_path, \"best_checkpoint.pt\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "\n",
    "    es_checker = EarlyStopping(\n",
    "        args[\"es_patience\"], True, args[\"es_delta\"], path=best_checkpt_path\n",
    "    )\n",
    "    writer = SummaryWriter(output_path)\n",
    "    metrics_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"epoch\",\n",
    "            \"metric_type\",\n",
    "            \"loss\",\n",
    "            \"accuracy\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"f1\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for epoch in range(args[\"num_epochs\"]):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_train_predictions = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        # Wrap train_loader with tqdm for progress visualization\n",
    "        train_loader_with_progress = tqdm(\n",
    "            train_loader, desc=f'Epoch {epoch+1}/{args[\"num_epochs\"]}', leave=False\n",
    "        )\n",
    "\n",
    "        for inputs, labels in train_loader_with_progress:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_train_predictions.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loader_with_progress.set_postfix(\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"acc\": (predicted == labels).sum().item() / labels.size(0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_metrics = calculate_metrics(\n",
    "            epoch, \"train\", train_loss, all_train_labels, all_train_predictions\n",
    "        )\n",
    "\n",
    "        new_df = pd.DataFrame.from_dict(train_metrics)\n",
    "        metrics_df = pd.concat([metrics_df, new_df], ignore_index=True)\n",
    "\n",
    "        log_metrics(writer, train_metrics)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_val_predictions = []\n",
    "        all_val_labels = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_val_predictions.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        val_metrics = calculate_metrics(\n",
    "            epoch, \"val\", val_loss, all_val_labels, all_val_predictions\n",
    "        )\n",
    "\n",
    "        new_df = pd.DataFrame.from_dict(val_metrics)\n",
    "        metrics_df = pd.concat([metrics_df, new_df], ignore_index=True)\n",
    "\n",
    "        log_metrics(writer, val_metrics)\n",
    "        print(f\"Epoch {epoch+1}/{args['num_epochs']}\")\n",
    "\n",
    "        # save checkpoints\n",
    "        if (epoch + 1) % args[\"checkpt_steps\"] == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(checkpoint_path, f\"checkpoint_{epoch+1}.pt\"),\n",
    "            )\n",
    "\n",
    "        es_checker(val_loss, model)\n",
    "        if es_checker.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    # calculate metrics on the test set, and log with the params\n",
    "    test_metrics = evaluate_model(model, criterion, test_loader, device)\n",
    "    writer.add_hparams(args, test_metrics)\n",
    "\n",
    "    visualize_metrics(metrics_df, output_path, timestamped_run_name)\n",
    "    metrics_df.to_pickle(os.path.join(output_path, \"metrics.pkl\"))\n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(best_checkpt_path)\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_path, \"last_checkpoint.pt\"))\n",
    "    writer.close()\n",
    "    return model, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\YOLO_v8\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Conda\\envs\\YOLO_v8\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = get_lr_scheduler(args[\"lr_scheduler_type\"], optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Validation loss decreased (inf --> 0.741019).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_run_brain_tumor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 96\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(run_name, model, criterion, optimizer, device, args)\u001b[0m\n\u001b[0;32m     94\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 96\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     99\u001b[0m val_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results = train_model(\n",
    "    model=model,\n",
    "    run_name='test_run_brain_tumor',\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in d:\\conda\\envs\\yolo_v8\\lib\\site-packages (from tensorboardX) (1.26.2)\n",
      "Requirement already satisfied: packaging in d:\\conda\\envs\\yolo_v8\\lib\\site-packages (from tensorboardX) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in d:\\conda\\envs\\yolo_v8\\lib\\site-packages (from tensorboardX) (4.25.3)\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/101.7 kB ? eta -:--:--\n",
      "   --------------- ----------------------- 41.0/101.7 kB 495.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 101.7/101.7 kB 973.0 kB/s eta 0:00:00\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.6.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboardX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = results[results.metric_type == 'train']\n",
    "val_res = results[results.metric_type == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_res['epoch'],train_res['accuracy'], label='train_accuracy')\n",
    "plt.plot(val_res['epoch'],val_res['accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_res['epoch'],train_res['loss'], label='train_loss')\n",
    "plt.plot(val_res['epoch'],val_res['loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "\n",
    "model.load_state_dict(torch.load('best_checkpoint.pt'))\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "scheduler = get_lr_scheduler(args[\"lr_scheduler_type\"], optimizer)\n",
    "es_checker = EarlyStopping(args[\"es_patience\"], True, args[\"es_delta\"], path=\"best_full_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/23 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 256 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results = train_model(model, criterion, optimizer, es_checker, num_epochs=args['epoch'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get predictions for test set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_predictions, classes=test_dataset.classes)\n",
    "\n",
    "# Show some results\n",
    "# You can print some images with their predicted and true labels to visually inspect the performance of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have defined your model, criterion, and test_loader previously\n",
    "writer = SummaryWriter('runs')\n",
    "for i in range(5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "    writer.add_scalar('val_loss', val_loss, i)\n",
    "    #writer.add_scalar('Train/Accuracy', train_accuracy, epoch)\n",
    "\n",
    "# # Calculate metrics\n",
    "# metrics = calculate_metrics(all_labels, all_predictions)\n",
    "# metrics['loss'] = val_loss\n",
    "\n",
    "# # Convert metrics to DataFrame\n",
    "# metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "\n",
    "# print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs')\n",
    "for i in range(10):\n",
    "    writer.add_scalar('alma', i, i)\n",
    "\n",
    "writer.flush()\n",
    "writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('asd')\n",
    "x = torch.arange(-5, 5, 0.1).view(-1, 1)\n",
    "y = -5 * x + 0.1 * torch.randn(x.size())\n",
    "\n",
    "model = torch.nn.Linear(1, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "def train_model(iter):\n",
    "    for epoch in range(iter):\n",
    "        y1 = model(x)\n",
    "        loss = criterion(y1, y)\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train_model(10)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_metrics_data(num_epochs):\n",
    "    metrics_data = []\n",
    "    epoch_range = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    for epoch in epoch_range:\n",
    "        # Generate train metrics with noise\n",
    "        train_loss = 1.0 / epoch + np.random.uniform(-0.05, 0.05)\n",
    "        train_accuracy = (0.8 + 0.02 * epoch) + np.random.uniform(-0.02, 0.02)\n",
    "        train_precision = (0.75 + 0.02 * epoch) + np.random.uniform(-0.02, 0.02)\n",
    "        train_recall = (0.85 + 0.01 * epoch) + np.random.uniform(-0.01, 0.01)\n",
    "        train_f1 = (0.79 + 0.015 * epoch) + np.random.uniform(-0.015, 0.015)\n",
    "\n",
    "        # Generate validation metrics with noise\n",
    "        val_loss = train_loss * 0.9 + np.random.uniform(-0.02, 0.02)\n",
    "        val_accuracy = (train_accuracy * 1.02) + np.random.uniform(-0.02, 0.02)\n",
    "        val_precision = (train_precision * 1.01) + np.random.uniform(-0.02, 0.02)\n",
    "        val_recall = (train_recall * 1.01) + np.random.uniform(-0.01, 0.01)\n",
    "        val_f1 = (train_f1 * 1.015) + np.random.uniform(-0.015, 0.015)\n",
    "\n",
    "        # Append train metrics\n",
    "        metrics_data.append({'epoch': epoch, 'metric_type': 'train', 'loss': train_loss,\n",
    "                             'accuracy': train_accuracy, 'precision': train_precision,\n",
    "                             'recall': train_recall, 'f1': train_f1})\n",
    "\n",
    "        # Append validation metrics\n",
    "        metrics_data.append({'epoch': epoch, 'metric_type': 'val', 'loss': val_loss,\n",
    "                             'accuracy': val_accuracy, 'precision': val_precision,\n",
    "                             'recall': val_recall, 'f1': val_f1})\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    return metrics_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "num_epochs = 50\n",
    "metrics_df = generate_metrics_data(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that takes this kind of daataframe and makes a visualisation out of this. It should create a grid wiht 3 columns and 2 rows. Each cell in the grid should have a plot of a metric listed in the df. (loss, accuracy, precision, recall f1). In each cell the corresponding metric should be visualized with a lineplot, both the train and the val type of metric in the same cell. Make the have a legend, with a train_ or val_ prefix.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
